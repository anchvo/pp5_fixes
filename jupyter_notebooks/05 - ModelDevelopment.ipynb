{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **MODEL DEVELOPMENT NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write here your notebook objective, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Set Project Root Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Centralise the base path using project_root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Resolve the project root\n",
        "project_root = Path.cwd()\n",
        "if project_root.name == \"jupyter_notebooks\":\n",
        "    project_root = project_root.parent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, all necessary standard libaries are imported to allow using their functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries with necessary Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Settings\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ML libraries\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Train & Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, the train and test sets are loaded to be able to access the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = project_root / \"outputs\" / \"data\"\n",
        "\n",
        "X_train = pd.read_csv(data_path / \"X_train.csv\")\n",
        "X_test = pd.read_csv(data_path / \"X_test.csv\")\n",
        "y_train = pd.read_csv(data_path / \"y_train.csv\").values.ravel()\n",
        "y_test = pd.read_csv(data_path / \"y_test.csv\").values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Data Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save train/test splits\n",
        "data_path = project_root / \"outputs\" / \"data\"\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "X_train_resampled.to_csv(data_path / \"X_train_resampled.csv\", index=False)\n",
        "\n",
        "# Convert numpy array before saving\n",
        "pd.Series(y_train_resampled).to_csv(data_path / \"y_train_resampled.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved resampled train sets to outputs/data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardise Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale X train and test set\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_arr = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled_arr = scaler.transform(X_test)\n",
        "\n",
        "# Convert numpy array to pandas df\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled_arr, columns=X_train_resampled.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled_arr, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"✅ X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"✅ y_train_resampled:\", y_train_resampled.shape)\n",
        "print(\"✅ X_test_scaled:\", X_test_scaled.shape)\n",
        "print(\"✅ y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Scale X_train_resampled and X_test for Logistic Regression and LinearSVC\n",
        "* Not for Random Forest or XGBoost, which are tree-based and scale-invariant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save scaled test and train sets\n",
        "data_path = project_root / \"outputs\" / \"data\"\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "X_train_scaled.to_csv(data_path / \"X_train_scaled.csv\", index=False)\n",
        "X_test_scaled.to_csv(data_path / \"X_test_scaled.csv\", index=False)\n",
        "\n",
        "print(\"✅ Saved scaled train and test set to outputs/data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function for Result Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "def collect_results(model_name, acc, balanced_acc, f1_macro, f1_weighted):\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': round(acc, 4),\n",
        "        'Balanced Accuracy': round(balanced_acc, 4),\n",
        "        'F1 Macro': round(f1_macro, 4),\n",
        "        'F1 Weighted': round(f1_weighted, 4)\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Function for Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and quickly evaluate any given model\n",
        "def train_and_evaluate(model, model_name, X_train_input, y_train_input,\n",
        "                       X_test_input, y_test_input, balance_test=False):\n",
        "    print(f\"\\n🧪 {model_name}\")\n",
        "\n",
        "    # Optional: balance test set\n",
        "    if balance_test:\n",
        "        ros = RandomOverSampler(random_state=42)\n",
        "        X_test_input, y_test_input = ros.fit_resample(X_test_input, y_test_input)\n",
        "        print(\"🔁 Test set was balanced for evaluation\")\n",
        "\n",
        "    # Train\n",
        "    model.fit(X_train_input, y_train_input)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(X_test_input)\n",
        "\n",
        "    # Suppress UndefinedMetricWarning for clean output\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(y_test_input, predictions)\n",
        "        balanced_acc = balanced_accuracy_score(y_test_input, predictions)\n",
        "        f1_macro = f1_score(y_test_input, predictions, average='macro')\n",
        "        f1_weighted = f1_score(y_test_input, predictions, average='weighted')\n",
        "\n",
        "        # Output\n",
        "        print(\"✅ Accuracy:\", round(acc, 4))\n",
        "        print(\"🎯 Balanced Accuracy:\", round(balanced_acc, 4))\n",
        "        print(\"🧮 F1 Score (macro):\", round(f1_macro, 4))\n",
        "        print(\"🧮 F1 Score (weighted):\", round(f1_weighted, 4))\n",
        "        print(\"📉 Confusion Matrix:\\n\", confusion_matrix(y_test_input, predictions))\n",
        "        print(\"📋 Classification Report:\\n\", classification_report(y_test_input, predictions))\n",
        "\n",
        "        # Collect results function\n",
        "        collect_results(model_name, acc, balanced_acc, f1_macro, f1_weighted)\n",
        "\n",
        "    return model, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Multiple Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start, multiple models are trained with different algorithms to find the best one for this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=2000, random_state=42)\n",
        "train_and_evaluate(logreg, \"Logistic Regression\", X_train_scaled, y_train_resampled,\n",
        "                   X_test_scaled, y_test,\n",
        "                   balance_test=True)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_and_evaluate(rf, \"Random Forest\", X_train_resampled, y_train_resampled,\n",
        "                   X_test, y_test,\n",
        "                   balance_test=False)\n",
        "\n",
        "# Linear Support Vector Classifier\n",
        "linear_svc = LinearSVC(max_iter=10000)\n",
        "train_and_evaluate(linear_svc, \"Linear Support Vector Classifier\", X_train_scaled, y_train_resampled,\n",
        "                   X_test_scaled, y_test,\n",
        "                   balance_test=True)\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "train_and_evaluate(xgb, \"XGBoost\", X_train_resampled, y_train_resampled,\n",
        "                   X_test, y_test,\n",
        "                   balance_test=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model Training Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_training_results = pd.DataFrame(results)\n",
        "\n",
        "evaluation_path = project_root / \"outputs\" / \"evaluation\"\n",
        "os.makedirs(evaluation_path, exist_ok=True)\n",
        "\n",
        "df_training_results.to_csv(evaluation_path / \"model_training_results.csv\", index=False)\n",
        "print(\"📁 Results saved to model_training_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Top 2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_path = project_root / \"outputs\" / \"ml_pipeline\"\n",
        "os.makedirs(ml_path, exist_ok=True)\n",
        "\n",
        "joblib.dump(rf, ml_path / \"default_rf.pkl\")\n",
        "joblib.dump(xgb, ml_path / \"default_xgb.pkl\")\n",
        "print(\"📁 Top 2 default models saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter grids\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [6, 10],\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Grid search for Random Forest\n",
        "print(\"🔍 Tuning Random Forest...\")\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_params,\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_grid.fit(X_train_resampled, y_train_resampled)\n",
        "tuned_rf = rf_grid.best_estimator_\n",
        "print(\"✅ Best RF Parameters:\", rf_grid.best_params_)\n",
        "\n",
        "# Grid search for XGBoost\n",
        "print(\"🔍 Tuning XGBoost...\")\n",
        "xgb_grid = GridSearchCV(\n",
        "    XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=42),\n",
        "    xgb_params,\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_grid.fit(X_train_resampled, y_train_resampled)\n",
        "tuned_xgb = xgb_grid.best_estimator_\n",
        "print(\"✅ Best XGB Parameters:\", xgb_grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Results after Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use helper function to train and evaluate models after tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Evaluate tuned RF\n",
        "tuned_rf = rf_grid.best_estimator_\n",
        "res_rf = train_and_evaluate(tuned_rf, \"Tuned Random Forest\", X_train_resampled, y_train_resampled, X_test, y_test)\n",
        "\n",
        "# Evaluate tuned XGB\n",
        "tuned_xgb = xgb_grid.best_estimator_\n",
        "res_xgb = train_and_evaluate(tuned_xgb, \"Tuned XGBoost\", X_train_resampled, y_train_resampled, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Evaluation Results and Tuned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "df_tuning_results = pd.DataFrame(results)\n",
        "\n",
        "evaluation_path = project_root / \"outputs\" / \"evaluation\"\n",
        "os.makedirs(evaluation_path, exist_ok=True)\n",
        "\n",
        "df_tuning_results.to_csv(evaluation_path / \"model_tuning_results.csv\", index=False)\n",
        "print(\"📁 Results saved to model_tuning_results.csv\")\n",
        "\n",
        "# Save tuned models\n",
        "ml_path = project_root / \"outputs\" / \"ml_pipeline\"\n",
        "os.makedirs(ml_path, exist_ok=True)\n",
        "\n",
        "joblib.dump(tuned_rf, ml_path / \"tuned_rf.pkl\")\n",
        "joblib.dump(tuned_xgb, ml_path / \"tuned_xgb.pkl\")\n",
        "print(\"📁 Top 2 tuned models saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Best Performing Default Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set path\n",
        "final_model_path = project_root / \"outputs\" / \"ml_pipeline\"\n",
        "os.makedirs(final_model_path, exist_ok=True)\n",
        "\n",
        "# Save tuned RF as current best model\n",
        "joblib.dump(tuned_rf, final_model_path / \"default_best_model.pkl\")\n",
        "print(\"📁 Best tuned RF model saved as default_best_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names from X_train_resampled\n",
        "feature_names = X_train_resampled.columns\n",
        "\n",
        "# Extract importances\n",
        "importances = tuned_rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame\n",
        "df_feature = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Plot top 15 features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(df_feature['feature'][:15][::-1], df_feature['importance'][:15][::-1])\n",
        "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save plot\n",
        "figures_path = project_root / \"outputs\" / \"eda\" / \"figures\"\n",
        "os.makedirs(figures_path, exist_ok=True)\n",
        "plt.savefig(figures_path / \"best_model_feature_importance.png\", dpi=300)\n",
        "print(\"📁 Saved best model feature importance to outputs/eda/figures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop features with importance below a threshold near zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop features with importance below a threshold\n",
        "low_imp_feats = df_feature[df_feature['importance'] < 0.001]['feature']\n",
        "X_train_fe = X_train_resampled.drop(columns=low_imp_feats)\n",
        "X_test_fe = X_test_scaled.drop(columns=low_imp_feats)\n",
        "\n",
        "print(f\"Dropped {len(low_imp_feats)} low-importance features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrain Default Best Model with Advanced Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Retrain on new features\n",
        "feature_rf = RandomForestClassifier(**tuned_rf.get_params())\n",
        "\n",
        "# Fit model\n",
        "feature_rf.fit(X_train_fe, y_train_resampled)\n",
        "\n",
        "# Evaluate\n",
        "train_and_evaluate(feature_rf, \"Random Forest + FE\", X_train_fe, y_train_resampled, X_test_fe, y_test, balance_test=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The feature engineering (feature dropping) didn't improve performance and significantly worsened it, meaning those “low importance” features likely had:\n",
        "\n",
        "* Small but meaningful contributions\n",
        "\n",
        "* Interactions with other features\n",
        "\n",
        "* Redundant but stabilizing effects on splits\n",
        "\n",
        "* Conclusion: Feature Engineering and removing features with low importance did not increase model performance. The original default_best_model can be considered the best_model with the best performance. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "df_fe_results = pd.DataFrame(results)\n",
        "\n",
        "evaluation_path = project_root / \"outputs\" / \"evaluation\"\n",
        "os.makedirs(evaluation_path, exist_ok=True)\n",
        "\n",
        "df_fe_results.to_csv(evaluation_path / \"best_model_fe_results.csv\", index=False)\n",
        "print(\"📁 Results saved to best_model_fe_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save feature engineered rf model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set path\n",
        "model_path = project_root / \"outputs\" / \"ml_pipeline\"\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# Save tuned RF as current best model\n",
        "joblib.dump(feature_rf, model_path / \"feature_rf.pkl\")\n",
        "print(\"📁 Best tuned RF model saved as feature_rf.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Final Model with Original Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load default best model to get clean version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = project_root / \"outputs\" / \"ml_pipeline\" / \"default_best_model.pkl\"\n",
        "defaul_best_model = joblib.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate default best model with original test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Evaluate on original test set\n",
        "train_and_evaluate(\n",
        "    defaul_best_model,\n",
        "    model_name=\"Final Random Forest (Test Set)\",\n",
        "    X_train_input=X_train_resampled,\n",
        "    y_train_input=y_train_resampled,\n",
        "    X_test_input=X_test_scaled,\n",
        "    y_test_input=y_test,\n",
        "    balance_test=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Poor performance reasoning and conclusion: \n",
        "* The model was trained on a resampled (balanced) training set, but the test set remains imbalanced.\n",
        "* The model learned to expect classes in roughly equal frequency.\n",
        "* When evaluated on the real-world imbalanced test set, its predictions don’t align well, especially for minority classes like class 2.\n",
        "* The model has zero precision and recall for class 2, meaning it's never predicting it. This could be due to ittle distinguishing signal for that class, overlapping features across classe or underrepresentation in test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "df_final_results = pd.DataFrame(results)\n",
        "\n",
        "evaluation_path = project_root / \"outputs\" / \"evaluation\"\n",
        "os.makedirs(evaluation_path, exist_ok=True)\n",
        "\n",
        "df_final_results.to_csv(evaluation_path / \"best_model_final_results.csv\", index=False)\n",
        "print(\"📁 Results saved to best_model_final_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
