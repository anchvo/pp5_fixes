{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **DATA CLEANING NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Given the results of the previous EDA, clean the data to solve issues and create a consistent and more balanced dataset for use in model training.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Android_Malware_converted.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Android_Malware_cleaned.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Set Project Root Directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Centralise the base path using project_root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Resolve the project root\n",
        "project_root = Path.cwd()\n",
        "if project_root.name == \"jupyter_notebooks\":\n",
        "    project_root = project_root.parent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, all necessary standard libaries are imported to allow using their functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries with necessary Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Settings\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Converted Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, the converted Dataset is loaded to be able to access the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_converted = pd.read_csv(Path.cwd().parent / \"outputs\" / \"data\" / \"Android_Malware_converted.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, the data is cleaned based on the issues found in the EDA. Additional evaluations of the data is performed to get a deeper insight into how to handle the issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Copy Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make a copy to preserve the original converted dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clean = df_converted.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The evaluation of missing data is done via a custom EvaluateMissingData function. This has been adapted from a Code Institute Walkthrough.\n",
        "* The function was adapted to be more sensitive and to avoid dropping variables that only look \"clean\" due to rounding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use custom function to evaluate missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Include minimum percentage of missing data required for a column to be included in the output\n",
        "def EvaluateMissingData(df_clean, min_threshold=0.0):\n",
        "\n",
        "    # Total missing values per column\n",
        "    missing_data_absolute = df_clean.isnull().sum()\n",
        "    \n",
        "    # Unrounded percentage calculation\n",
        "    missing_data_percentage = (missing_data_absolute / len(df_clean)) * 100\n",
        "\n",
        "    # Create the summary dataframe\n",
        "    df_missing_data = (\n",
        "        pd.DataFrame({\n",
        "            \"RowsWithMissingData\": missing_data_absolute,\n",
        "            \"PercentageOfDataset\": missing_data_percentage,\n",
        "            \"DataType\": df_clean.dtypes\n",
        "        })\n",
        "        .sort_values(by=\"PercentageOfDataset\", ascending=False)\n",
        "        .query(f\"PercentageOfDataset > {min_threshold}\")\n",
        "    )\n",
        "\n",
        "    # Round only for display\n",
        "    df_missing_data[\"PercentageOfDataset\"] = df_missing_data[\"PercentageOfDataset\"].round(2)\n",
        "\n",
        "    return df_missing_data\n",
        "\n",
        "# Display summary dataframe\n",
        "df_missing_data = EvaluateMissingData(df_clean, min_threshold=0.001)\n",
        "display(df_missing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* From previous EDA some features are known to have all-zero values (and zero outliers)\n",
        "* These features can be dropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drop selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "features_to_drop = [\n",
        "    'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
        "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'\n",
        "]\n",
        "\n",
        "dropper = DropFeatures(features_to_drop=features_to_drop)\n",
        "dropper.fit(df_clean)\n",
        "df_clean = dropper.transform(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rerun EvaluateMissingData function to check improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* From previous EDA and evaluating missing data, the amount and type of missing data is known.\n",
        "* Because of the type of features, numerical and continuos, median imputation can be useful for handling missing data.\n",
        "* This approach is also robust to outliers/skewing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Impute missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
        "\n",
        "# For numeric variables with missing values\n",
        "numeric_vars = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
        "numeric_with_na = [col for col in numeric_vars if df_clean[col].isna().sum() > 0]\n",
        "\n",
        "median_imputer = MeanMedianImputer(imputation_method='median', variables=numeric_with_na)\n",
        "median_imputer.fit(df_clean)\n",
        "df_clean = median_imputer.transform(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle Skewed Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In previous EDA, many features with very high skewedness were found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use np.log1p() for skewed positive variables and store transformed column names for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Find highly skewed numeric features\n",
        "skewed_features = df_clean[numeric_vars].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "high_skew = skewed_features[abs(skewed_features) > 2].index.tolist()\n",
        "\n",
        "# Apply log1p transformation (avoids negative values)\n",
        "def safe_log1p(x):\n",
        "    return np.log1p(np.where(x < 0, 0, x))\n",
        "\n",
        "df_clean[high_skew] = df_clean[high_skew].apply(safe_log1p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using log1p may result in infinite values, which need to be replaced and imputed again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Impute again for possible NaNs introduced via log1p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace any remaining infinite values with NaN (may result from log1p)\n",
        "df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Impute again (if log1p introduced NaNs)\n",
        "numeric_with_na = [col for col in df_clean.columns if df_clean[col].isna().sum() > 0]\n",
        "if numeric_with_na:\n",
        "    median_imputer = MeanMedianImputer(imputation_method='median', variables=numeric_with_na)\n",
        "    median_imputer.fit(df_clean)\n",
        "    df_clean = median_imputer.transform(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run final check for uncleaned data with custom EvaluateMissingData function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df_clean)\n",
        "\n",
        "# Check for remaining skew, outlier ranges\n",
        "df_clean.describe().T  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save cleaned dataframe as csv file for better access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned dataframe as csv file for easier access of cleaned data\n",
        "data_path = project_root / \"outputs\" / \"data\"\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "df_clean.to_csv(data_path / \"Android_Malware_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Saved cleaned dataframe to outputs/data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The data is now cleaned and ready to be used in feature engineering for training the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
